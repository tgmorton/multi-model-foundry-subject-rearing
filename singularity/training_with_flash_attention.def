Bootstrap: docker
From: ubuntu:22.04

%files
    # Only copy the requirements.txt file, which is needed for the build.
    requirements.txt /requirements.txt

%environment
    # Set standard environment variables inside the container
    export LC_ALL=C.UTF-8
    export LANG=C.UTF-8
    export PYTHONUNBUFFERED=1
    # Set CUDA_HOME for flash-attention compilation
    export CUDA_HOME=/usr/local/cuda
    export PATH=${CUDA_HOME}/bin:${PATH}
    export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

%post
    # --- 1. Install System Dependencies ---
    echo "Updating packages and installing system dependencies..."
    apt-get update && apt-get install -y --no-install-recommends \
        build-essential \
        git \
        wget \
        curl \
        ca-certificates \
        software-properties-common \
        gnupg2 \
        python3 \
        python3-dev \
        python3-distutils \
        python3-pip

    # Set python3 as default python (Ubuntu 22.04 comes with Python 3.10)
    update-alternatives --install /usr/bin/python python /usr/bin/python3 1

    # --- 2. Install CUDA 11.8 ---
    echo "Installing CUDA 11.8..."
    # Add NVIDIA package repositories
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
    dpkg -i cuda-keyring_1.0-1_all.deb
    apt-get update
    
    # Install CUDA 11.8 toolkit
    apt-get install -y --no-install-recommends \
        cuda-toolkit-11-8 \
        cuda-compiler-11-8 \
        cuda-libraries-dev-11-8 \
        cuda-driver-dev-11-8

    # Set up CUDA symlinks
    ln -sf /usr/local/cuda-11.8 /usr/local/cuda

    # --- 3. Install Python Packages ---
    echo "Upgrading pip..."
    python3 -m pip install --upgrade pip wheel setuptools

    echo "Installing PyTorch with CUDA support..."
    pip install --no-cache-dir torch==2.1.0 --index-url https://download.pytorch.org/whl/cu118

    echo "Installing Python requirements from /requirements.txt..."
    pip install --no-cache-dir -r /requirements.txt

    echo "Installing flash-attention..."
    # Install flash-attention with proper CUDA environment
    export CUDA_HOME=/usr/local/cuda
    export PATH=${CUDA_HOME}/bin:${PATH}
    export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}
    pip install --no-cache-dir flash-attn --no-build-isolation

    echo "Downloading NLTK data..."
    python3 -m nltk.downloader punkt

    # --- 4. Clean Up (but keep build tools for potential runtime compilation) ---
    echo "Cleaning up package cache..."
    apt-get clean
    rm -rf /var/lib/apt/lists/*
    pip cache purge

    echo "Build post-install complete."

%labels
    Author Thomas Morton
    Version 2.0-flash-attention
    Python_Version 3.10
    CUDA_Version 11.8
    Flash_Attention Enabled

%runscript
   # This script runs when you execute `apptainer run <sif_file>`
   echo "Italian LLM Training Environment Container with Flash Attention"
   echo "-------------------------------------------------------------"
   echo "This container provides the environment with Flash Attention support."
   echo "Mount your project code to run it."
   echo "Example: apptainer exec --nv --bind .:/workspace <sif_file> python -m model_foundry.cli run config.yaml"