# ===================================================================
# EXPERIMENT 2: No Expletives + Poor Determiner Morphology
# GOAL: Test the combined effect of removing expletives and impoverishing
# [cite_start]the determiner system, relevant to Duguine's theory. [cite: 171, 174]
# ===================================================================

experiment_name: "exp2_no_expletives_poor_determiners"

# --- Data Configuration ---
data:
  source_corpus: "data/raw/train_90M/"
  # The final training corpus is the output of the final manipulation step.
  training_corpus: "data/processed/exp2_impoverish_determiners/"
  # Path to the test dataset.
  test_corpus: "data/raw/test_10M/"
  batch_size: 8  # Reduced from 32 to fix OOM
  max_sequence_length: 1000

# --- Dataset Manipulation Pipeline ---
# Dataset manipulation already completed - using existing processed data
dataset_manipulation: []

# --- Tokenizer Configuration ---
tokenizer:
  output_dir: "tokenizers/exp2_no_expletives_poor_determiners/"
  vocab_size: 50004

# --- Model Architecture ---
# Identical to the baseline.
model:
  layers: 12
  embedding_size: 768
  hidden_size: 768
  intermediate_hidden_size: 3072
  attention_heads: 12
  activation_function: "gelu"
  dropout: 0.1
  attention_dropout: 0.1

# --- Training Procedure ---
# Identical to the baseline.
training:
  output_dir: "models/exp2_no_expletives_poor_determiners/"
  learning_rate: 0.0004  # 4x higher for 4x smaller effective batch size
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-6
  # Training duration - automatically calculated from dataset size
  warmup_steps: null  # Auto-calculated as warmup_ratio * train_steps
  warmup_ratio: 0.1  # Warmup as 10% of total training steps
  train_steps: null  # Auto-calculated as epochs × steps_per_epoch
  epochs: 20
  
  # Mixed precision training
  use_amp: true  # Re-enabled to save memory
  
  # Gradient accumulation
  gradient_accumulation_steps: 32  # Effective batch size: 8 × 32 = 256 (same as before)
  
  # Gradient clipping for training stability
  max_grad_norm: 1.0
  
  # Memory optimizations
  use_tf32: true
  use_gradient_checkpointing: true
  
  # Flash Attention optimization
  use_flash_attention: true  # Re-enabled to save memory
  
  # Compilation mode for torch.compile
  # Options: 'none', 'default', 'reduce-overhead', 'max-autotune'
  # Use 'default' for stability, 'none' to disable
  compile_mode: 'none'
  
  # Checkpoint generation parameters
  auto_generate_checkpoints: true
  first_epoch_checkpoints: 20
  subsequent_epochs_spacing: "log"
  log_base: 2
  min_checkpoint_interval: 100
  min_checkpoints_per_epoch: 5

# --- Logging & Reproducibility ---
logging:
  use_wandb: true
  wandb_project: "just-drop-the-subject"
random_seed: 9